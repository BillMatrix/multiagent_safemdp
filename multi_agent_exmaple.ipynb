{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 agents:\n",
    "\n",
    "Multiagent:\n",
    "Collide: 0.0 0.0\n",
    "Unsafe: 0.0 0.0\n",
    "\n",
    "Singleagent:\n",
    "Collide: 1.5 2.9410882339705484\n",
    "Unsafe: 0.0 0.0\n",
    "\n",
    "3 agents:\n",
    "Multiagent:\n",
    "0.0 0.0\n",
    "2.3 0.7810249675906654\n",
    "\n",
    "Singleagent:\n",
    "1.6 2.973213749463701\n",
    "2.0 0.6324555320336759\n",
    "\n",
    "4 agents:\n",
    "Multiagent\n",
    "0.1 0.30000000000000004\n",
    "1.0 0.6324555320336759\n",
    "\n",
    "Singleagent\n",
    "1.9 2.6248809496813377\n",
    "0.5 0.6708203932499369\n",
    "\n",
    "5 agents:\n",
    "Multiagent\n",
    "0.7 0.9\n",
    "47.6 0.9165151389911679\n",
    "\n",
    "Singleagent\n",
    "3.0 2.898275349237888\n",
    "46.8 0.4\n",
    "\n",
    "6 agents:\n",
    "Multiagent\n",
    "1.4 1.0198039027185568\n",
    "28.0 2.8635642126552705\n",
    "\n",
    "Singleagent\n",
    "6.5 3.0740852297878796\n",
    "26.5 1.91049731745428\n",
    "\n",
    "7 agents:\n",
    "Multiagent:\n",
    "1.5 1.3601470508735443\n",
    "11.5 2.5787593916455256\n",
    "\n",
    "Singleagent:\n",
    "6.8 1.6613247725836149\n",
    "8.3 1.3453624047073711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import time\n",
    "\n",
    "import GPy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from multiagent_grid_world import (compute_true_safe_set, compute_S_hat0,\n",
    "                                compute_true_S_hat, draw_gp_sample, MultiagentGridWorldAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agent = 2\n",
    "agent_explore_kernels = []\n",
    "agent_explore_liks = []\n",
    "# agent_exploit_kernels = []\n",
    "# agent_exploit_liks = []\n",
    "noise = 0.001\n",
    "\n",
    "for agent in range(num_agent):\n",
    "    agent_explore_kernels += [GPy.kern.RBF(input_dim=4, lengthscale=(2., 2., 2., 2.), ARD=True)]\n",
    "    agent_explore_liks += [GPy.likelihoods.Gaussian(variance=noise ** 2)]\n",
    "    agent_explore_liks[agent].constrain_bounded(1e-6, 1)\n",
    "    \n",
    "# for agent in range(num_agent):\n",
    "#     agent_exploit_kernels += [GPy.kern.RBF(input_dim=4, lengthscale=(2., 2., 2., 2.), ARD=True)]\n",
    "#     agent_exploit_liks += [GPy.likelihoods.Gaussian(variance=noise ** 2)]\n",
    "#     agent_exploit_liks[agent].constrain_bounded(1e-6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_rewards_kernels = []\n",
    "agent_rewards_liks = []\n",
    "noise = 0.001\n",
    "\n",
    "for agent in range(num_agent):\n",
    "    agent_rewards_kernels += [GPy.kern.RBF(input_dim=2, lengthscale=(2., 2.), variance=1., ARD=True)]\n",
    "    agent_rewards_liks += [GPy.likelihoods.Gaussian(variance=noise ** 2)]\n",
    "    agent_rewards_liks[agent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define world\n",
    "world_shape = (5, 5)\n",
    "step_size = (0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define coordinates\n",
    "n, m = world_shape\n",
    "step1, step2 = step_size\n",
    "xx, yy = np.meshgrid(np.linspace(0, (n - 1) * step1, n),\n",
    "                     np.linspace(0, (m - 1) * step2, m),\n",
    "                     indexing=\"ij\")\n",
    "coord = np.vstack((xx.flatten(), yy.flatten())).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety threhsold\n",
    "h = -0.25\n",
    "\n",
    "# Lipschitz\n",
    "L = 0\n",
    "\n",
    "# Scaling factor for confidence interval\n",
    "beta = 2\n",
    "\n",
    "collide_threshold = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reconstraining parameters Gaussian_noise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02440064 -0.19850443 -0.55144736 -0.99568087 -1.46362983  0.43057205\n",
      "  0.25993253 -0.04180778 -0.4524123  -0.92367692  0.74268053  0.63671968\n",
      "  0.40978713  0.06478553 -0.37616521  0.80686004  0.77718224  0.65093172\n",
      "  0.4119865   0.05280927  0.57692184  0.6357917   0.63521296  0.54122631\n",
      "  0.31646203]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# Data to initialize GP\n",
    "agent_gps = []\n",
    "agent_rewards_gps = []\n",
    "altitude_kernel = GPy.kern.RBF(input_dim=2, lengthscale=(2., 2.), variance=1., ARD=True)\n",
    "altitude_lik = GPy.likelihoods.Gaussian(variance=noise ** 2)\n",
    "altitude_lik.constrain_bounded(1e-6, 10000.)\n",
    "altitudes, coord = draw_gp_sample(altitude_kernel, world_shape, step_size)\n",
    "print(altitudes)\n",
    "\n",
    "for agent in range(num_agent):\n",
    "    noise = 0.001\n",
    "    n_samples = 1\n",
    "    ind = np.random.choice(range(altitudes.size), n_samples)\n",
    "    X = coord[ind, :]\n",
    "    Y = altitudes[ind].reshape(n_samples, 1) + np.random.randn(n_samples, 1)\n",
    "    agent_gps += [GPy.core.GP(X, Y, copy.deepcopy(altitude_kernel), copy.deepcopy(altitude_lik))]\n",
    "    agent_rewards_gps += [GPy.core.GP(X, Y, copy.deepcopy(altitude_kernel), copy.deepcopy(altitude_lik))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_samples = 1\n",
    "agent_explore_gps = []\n",
    "# agent_exploit_gps = []\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for agent in range(num_agent):\n",
    "    for i in range(world_shape[0]):\n",
    "        for j in range(world_shape[1]):\n",
    "            for i_ in range(world_shape[0]):\n",
    "                for j_ in range(world_shape[1]):\n",
    "                    X += [[i * step_size[0], j * step_size[1], i_ * step_size[0], j_ * step_size[1]]]\n",
    "                    Y += [[np.random.uniform(-1,0,1000)[0]]]\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    mean_func_explore = GPy.core.Mapping(4, 1, name='agent_explore')\n",
    "    mean_func_explore.f = lambda x: 0.5\n",
    "    mean_func_explore.update_gradients = lambda a,b: 0\n",
    "    mean_func_explore.gradients_X = lambda a,b: 0\n",
    "    agent_explore_gps += [GPy.core.GP(X, Y, agent_explore_kernels[agent], agent_explore_liks[agent], mean_function=mean_func_explore)]\n",
    "    \n",
    "#     mean_func_exploit = GPy.core.Mapping(4, 1, name='agent_exploit')\n",
    "#     mean_func_exploit.f = lambda x: 0.5\n",
    "#     mean_func_exploit.update_gradients = lambda a,b: 0\n",
    "#     mean_func_exploit.gradients_X = lambda a,b: 0\n",
    "#     agent_exploit_gps += [GPy.core.GP(X, Y, agent_exploit_kernels[agent], agent_exploit_liks[agent], mean_function=mean_func_exploit)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Edges Added\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"float\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-11dcb19d19d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mcur_agent_pos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_agent\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             )\n\u001b[0;32m     53\u001b[0m         ]\n",
      "\u001b[1;32m~\\Documents\\stanford\\multiagent_safemdp\\multiagent_safemdp\\multiagent_grid_world.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, gp, others_explore_gp, others_rewards_gp, world_shape, step_size, beta, altitudes, h, collide_threshold, S0, S_hat0, my_pos_ind, L, other_pos, epsilons, update_dist, gamma)\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_other_agents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         ]\n\u001b[1;32m--> 308\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mplot_S\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\stanford\\multiagent_safemdp\\multiagent_safemdp\\multiagent_grid_world.py\u001b[0m in \u001b[0;36m_value_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    745\u001b[0m                         )[0]\n\u001b[0;32m    746\u001b[0m                         self.other_value_functions[agent][node] = max(\n\u001b[1;32m--> 747\u001b[1;33m                             \u001b[0mcur_reward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mother_value_functions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    748\u001b[0m                             \u001b[0mold_value_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m                         )\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate tuple (not \"float\") to tuple"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from tqdm import tqdm\n",
    "collides = []\n",
    "unsafes = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(epoch)\n",
    "    count_collide = 0\n",
    "    count_unsafe = 0\n",
    "    agents = []\n",
    "    agent_S0s = []\n",
    "    agent_S_hat0s = []\n",
    "    agent_pos = []\n",
    "    while len(agent_pos) != num_agent:\n",
    "        agent_pos = []\n",
    "        for agent in range(num_agent):\n",
    "            S0 = np.zeros((np.prod(world_shape), 5), dtype=bool)\n",
    "            S0[:, 0] = True\n",
    "            S_hat0 = compute_S_hat0(np.nan, world_shape, 4, altitudes,\n",
    "                                    step_size, h)\n",
    "            for i in range(len(np.where(S_hat0[:, 1])[0])):\n",
    "                if np.where(S_hat0[:, 1])[0][i] not in agent_pos:\n",
    "                    agent_pos += [np.where(S_hat0[:, 1])[0][i]]\n",
    "                    break\n",
    "            agent_S0s += [S0]\n",
    "            agent_S_hat0s += [S_hat0]\n",
    "\n",
    "    for agent in range(num_agent):\n",
    "        cur_agent_explore_gps = copy.deepcopy(agent_explore_gps)\n",
    "        cur_agent_explore_gps.pop(agent)\n",
    "        cur_agent_rewards_gps = copy.deepcopy(agent_rewards_gps)\n",
    "        cur_agent_rewards_gps.pop(agent)\n",
    "        cur_agent_pos = copy.deepcopy(agent_pos)\n",
    "        cur_agent_pos.pop(agent)\n",
    "        agents += [\n",
    "            MultiagentGridWorldAgent(\n",
    "                agent_gps[agent], \n",
    "                cur_agent_explore_gps,\n",
    "                cur_agent_rewards_gps,\n",
    "                world_shape, \n",
    "                step_size, \n",
    "                beta, \n",
    "                altitudes, \n",
    "                h, \n",
    "                collide_threshold, \n",
    "                agent_S0s[agent],\n",
    "                agent_S_hat0s[agent], \n",
    "                agent_pos[agent], \n",
    "                L, \n",
    "                cur_agent_pos,\n",
    "                [0.5 for _ in range(num_agent - 1)],\n",
    "            )\n",
    "        ]\n",
    "\n",
    "#     for i in tqdm(range(10)):\n",
    "#         agent_actions = [0 for agent in range(num_agent)]\n",
    "#         agent_next_samples = []\n",
    "#         agent_states = []\n",
    "#         for agent in range(num_agent):\n",
    "#             agents[agent].update_sets()\n",
    "#             next_sample = agents[agent].target_sample()\n",
    "#     #         print('agent:' + str(agent))\n",
    "#     #         print(next_sample)\n",
    "#             agent_actions[agent] = next_sample[1]\n",
    "#             agent_next_samples += [next_sample]\n",
    "#             agent_states += [next_sample[0]]\n",
    "#             if altitudes[next_sample[0]] < h:\n",
    "#     #             print('entered an unsafe state')\n",
    "#                 count_unsafe += 1\n",
    "#     #         print()\n",
    "\n",
    "#     #     print(agent_states)\n",
    "#         if len(set(agent_states)) < len(agent_states):\n",
    "#             count_collide += 1\n",
    "#     #     print()\n",
    "\n",
    "#         for agent in range(num_agent):\n",
    "#             agents[agent].add_observation(agent_next_samples[agent][0], agent_next_samples[agent][1], agent_actions)\n",
    "            \n",
    "#     collides += [count_collide]\n",
    "#     unsafes += [count_unsafe]\n",
    "\n",
    "print(collides)\n",
    "print(unsafes)\n",
    "print(np.mean(collides), np.std(collides))\n",
    "print(np.mean(unsafes), np.std(unsafes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm import tqdm\n",
    "from singleagent_grid_world import SingleagentGridWorldAgent\n",
    "collides = []\n",
    "unsafes = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(epoch)\n",
    "    count_collide = 0\n",
    "    count_unsafe = 0\n",
    "    agents = []\n",
    "    agent_S0s = []\n",
    "    agent_S_hat0s = []\n",
    "    agent_pos = []\n",
    "    while len(agent_pos) != num_agent:\n",
    "        agent_pos = []\n",
    "        for agent in range(num_agent):\n",
    "            S0 = np.zeros((np.prod(world_shape), 5), dtype=bool)\n",
    "            S0[:, 0] = True\n",
    "            S_hat0 = compute_S_hat0(np.nan, world_shape, 4, altitudes,\n",
    "                                    step_size, h)\n",
    "            for i in range(len(np.where(S_hat0[:, 1])[0])):\n",
    "                if not np.where(S_hat0[:, 1])[0][i] in agent_pos:\n",
    "                    agent_pos += [np.where(S_hat0[:, 1])[0][i]]\n",
    "                    break\n",
    "            agent_S0s += [S0]\n",
    "            agent_S_hat0s += [S_hat0]\n",
    "\n",
    "    for agent in range(num_agent):\n",
    "        cur_agent_action_gps = copy.deepcopy(agent_action_gps)\n",
    "        cur_agent_action_gps.pop(agent)\n",
    "        cur_agent_pos = copy.deepcopy(agent_pos)\n",
    "        cur_agent_pos.pop(agent)\n",
    "        agents += [\n",
    "            SingleagentGridWorldAgent(\n",
    "                agent_gps[agent], \n",
    "                cur_agent_action_gps, \n",
    "                world_shape, \n",
    "                step_size, \n",
    "                beta, \n",
    "                altitudes, \n",
    "                h, \n",
    "                collide_threshold, \n",
    "                agent_S0s[agent],\n",
    "                agent_S_hat0s[agent], \n",
    "                agent_pos[agent], \n",
    "                L, \n",
    "                cur_agent_pos\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    for i in tqdm(range(100)):\n",
    "        agent_actions = [0 for agent in range(num_agent)]\n",
    "        agent_next_samples = []\n",
    "        agent_states = []\n",
    "        for agent in range(num_agent):\n",
    "            agents[agent].update_sets()\n",
    "            next_sample = agents[agent].target_sample()\n",
    "    #         print('agent:' + str(agent))\n",
    "    #         print(next_sample)\n",
    "            agent_actions[agent] = next_sample[1]\n",
    "            agent_next_samples += [next_sample]\n",
    "            agent_states += [next_sample[0]]\n",
    "            if altitudes[next_sample[0]] < h:\n",
    "    #             print('entered an unsafe state')\n",
    "                count_unsafe += 1\n",
    "    #         print()\n",
    "\n",
    "    #     print(agent_states)\n",
    "        if len(set(agent_states)) < len(agent_states):\n",
    "            count_collide += 1\n",
    "    #     print()\n",
    "\n",
    "        for agent in range(num_agent):\n",
    "            agents[agent].add_observation(agent_next_samples[agent][0], agent_next_samples[agent][1], agent_actions)\n",
    "            \n",
    "    collides += [count_collide]\n",
    "    unsafes += [count_unsafe]\n",
    "    \n",
    "print(collides)\n",
    "print(unsafes)\n",
    "print(np.mean(collides), np.std(collides))\n",
    "print(np.mean(unsafes), np.std(unsafes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
